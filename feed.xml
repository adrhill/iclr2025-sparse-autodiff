<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://iclr-blogposts.github.io/2025/feed.xml" rel="self" type="application/atom+xml"/><link href="https://iclr-blogposts.github.io/2025/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-23T03:59:13+08:00</updated><id>https://iclr-blogposts.github.io/2025/feed.xml</id><title type="html">ICLR Blogposts 2025</title><subtitle>Home to the 2025 ICLR Blogposts track </subtitle><entry><title type="html">Sample Blog Post</title><link href="https://iclr-blogposts.github.io/2025/blog/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post"/><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://iclr-blogposts.github.io/2025/blog/distill-example</id><content type="html" xml:base="https://iclr-blogposts.github.io/2025/blog/distill-example/"><![CDATA[<p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.html path="assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2025-04-28-[SUBMISSION NAME]</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/8.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/10.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/11.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/12.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="interactive-figures">Interactive Figures</h3> <p>Here’s how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work (<strong>no extra javascript is allowed!</strong>). All that’s required is for you to export your figure into HTML format, and make sure that the file exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory. To embed it into any page, simply insert the following code anywhere into your page.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %} 
</code></pre></div></div> <p>For example, the following code can be used to generate the figure underneath it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">./assets/html/2025-04-28-distill-example/plotly_demo_1.html</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>And then include it with the following:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span> <span class="na">src=</span><span class="s">"{{ 'assets/html/2025-04-28-distill-example/plotly_demo_1.html' | relative_url }}"</span> <span class="na">frameborder=</span><span class="s">'0'</span> <span class="na">scrolling=</span><span class="s">'no'</span> <span class="na">height=</span><span class="s">"600px"</span> <span class="na">width=</span><span class="s">"100%"</span><span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>
</code></pre></div></div> <p>Voila!</p> <div class="l-page"> <iframe src="/2025/assets/html/2025-04-28-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br/> code code code <br/> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <hr/> <h2 id="diagrams">Diagrams</h2> <p>This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> plugin. Below, we generate a few examples of such diagrams using languages such as <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a>, <a href="https://plantuml.com/" target="\_blank">plantuml</a>, <a href="https://vega.github.io/vega-lite/" target="\_blank">vega-lite</a>, etc.</p> <p><strong>Note:</strong> different diagram-generation packages require external dependencies to be installed on your machine. Also, be mindful of that because of diagram generation the first time you build your Jekyll website after adding new diagrams will be SLOW. For any other details, please refer to <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> README.</p> <p><strong>Note:</strong> This is not supported for local rendering!</p> <p>The diagram below was generated by the following code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
{% endmermaid %}
</code></pre></div></div> <div class="jekyll-diagrams diagrams mermaid"> <svg id="mermaid-1732305558463" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1732305558463 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1732305558463 .node circle,#mermaid-1732305558463 .node ellipse,#mermaid-1732305558463 .node polygon,#mermaid-1732305558463 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1732305558463 .node.clickable{cursor:pointer}#mermaid-1732305558463 .arrowheadPath{fill:#333}#mermaid-1732305558463 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1732305558463 .edgeLabel{background-color:#e8e8e8}#mermaid-1732305558463 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1732305558463 .cluster text{fill:#333}#mermaid-1732305558463 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1732305558463 .actor{stroke:#ccf;fill:#ececff}#mermaid-1732305558463 text.actor{fill:#000;stroke:none}#mermaid-1732305558463 .actor-line{stroke:grey}#mermaid-1732305558463 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1732305558463 .messageLine0,#mermaid-1732305558463 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1732305558463 #arrowhead{fill:#333}#mermaid-1732305558463 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1732305558463 .messageText{fill:#333;stroke:none}#mermaid-1732305558463 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1732305558463 .labelText,#mermaid-1732305558463 .loopText{fill:#000;stroke:none}#mermaid-1732305558463 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1732305558463 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1732305558463 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1732305558463 .section{stroke:none;opacity:.2}#mermaid-1732305558463 .section0{fill:rgba(102,102,255,.49)}#mermaid-1732305558463 .section2{fill:#fff400}#mermaid-1732305558463 .section1,#mermaid-1732305558463 .section3{fill:#fff;opacity:.2}#mermaid-1732305558463 .sectionTitle0,#mermaid-1732305558463 .sectionTitle1,#mermaid-1732305558463 .sectionTitle2,#mermaid-1732305558463 .sectionTitle3{fill:#333}#mermaid-1732305558463 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1732305558463 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1732305558463 .grid path{stroke-width:0}#mermaid-1732305558463 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1732305558463 .task{stroke-width:2}#mermaid-1732305558463 .taskText{text-anchor:middle;font-size:11px}#mermaid-1732305558463 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1732305558463 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1732305558463 .taskText0,#mermaid-1732305558463 .taskText1,#mermaid-1732305558463 .taskText2,#mermaid-1732305558463 .taskText3{fill:#fff}#mermaid-1732305558463 .task0,#mermaid-1732305558463 .task1,#mermaid-1732305558463 .task2,#mermaid-1732305558463 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1732305558463 .taskTextOutside0,#mermaid-1732305558463 .taskTextOutside1,#mermaid-1732305558463 .taskTextOutside2,#mermaid-1732305558463 .taskTextOutside3{fill:#000}#mermaid-1732305558463 .active0,#mermaid-1732305558463 .active1,#mermaid-1732305558463 .active2,#mermaid-1732305558463 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1732305558463 .activeText0,#mermaid-1732305558463 .activeText1,#mermaid-1732305558463 .activeText2,#mermaid-1732305558463 .activeText3{fill:#000!important}#mermaid-1732305558463 .done0,#mermaid-1732305558463 .done1,#mermaid-1732305558463 .done2,#mermaid-1732305558463 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1732305558463 .doneText0,#mermaid-1732305558463 .doneText1,#mermaid-1732305558463 .doneText2,#mermaid-1732305558463 .doneText3{fill:#000!important}#mermaid-1732305558463 .crit0,#mermaid-1732305558463 .crit1,#mermaid-1732305558463 .crit2,#mermaid-1732305558463 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1732305558463 .activeCrit0,#mermaid-1732305558463 .activeCrit1,#mermaid-1732305558463 .activeCrit2,#mermaid-1732305558463 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1732305558463 .doneCrit0,#mermaid-1732305558463 .doneCrit1,#mermaid-1732305558463 .doneCrit2,#mermaid-1732305558463 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1732305558463 .activeCritText0,#mermaid-1732305558463 .activeCritText1,#mermaid-1732305558463 .activeCritText2,#mermaid-1732305558463 .activeCritText3,#mermaid-1732305558463 .doneCritText0,#mermaid-1732305558463 .doneCritText1,#mermaid-1732305558463 .doneCritText2,#mermaid-1732305558463 .doneCritText3{fill:#000!important}#mermaid-1732305558463 .titleText{text-anchor:middle;font-size:18px;fill:#000}
#mermaid-1732305558463 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1732305558463 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1732305558463 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1732305558463 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1732305558463 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1732305558463 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1732305558463 #compositionEnd,#mermaid-1732305558463 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1732305558463 #aggregationEnd,#mermaid-1732305558463 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1732305558463 #dependencyEnd,#mermaid-1732305558463 #dependencyStart,#mermaid-1732305558463 #extensionEnd,#mermaid-1732305558463 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1732305558463 .branch-label,#mermaid-1732305558463 .commit-id,#mermaid-1732305558463 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1732305558463{color:#000;font:normal normal 400 normal 16px / normal "Times New Roman"}</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg> </div> <hr/> <h2 id="tweets">Tweets</h2> <p>An example of displaying a tweet:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>An example of pulling from a timeline:</p> <div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p> <hr/> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code>-sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item ⋅⋅* Unordered sub-list.</li> <li>Actual numbers don’t matter, just that it’s a number ⋅⋅1. Ordered sub-list</li> <li>And another item.</li> </ol> <p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅ ⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅ ⋅⋅⋅(This is contrary to the typical GFM line break behavior, where trailing spaces are not required.)</p> <ul> <li>Unordered lists can use asterisks</li> <li>Or minuses</li> <li>Or pluses</li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting. 
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry><entry><title type="html">Sample Blog Post (HTML version)</title><link href="https://iclr-blogposts.github.io/2025/blog/distill-example2/" rel="alternate" type="text/html" title="Sample Blog Post (HTML version)"/><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://iclr-blogposts.github.io/2025/blog/distill-example2</id><content type="html" xml:base="https://iclr-blogposts.github.io/2025/blog/distill-example2/"><![CDATA[<p> This is a sample blog post written in HTML (while the other <a href="/2025/blog/distill-example/">sample post</a> is written in Markdown). Authors have the choice to write in HTML or Markdown. While Markdown is easier to write, HTML gives you more control over the layout of your post. Furthermore, Markdown often interacts in unexpected ways with MathJax and other HTML widgets. If you are having trouble with Markdown, try writing in HTML instead. </p> <p> Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling. </p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code>$$</code>, like <code>$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code>$$</code> and place it as a separate paragraph. Here is an example: $$ \left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right) $$ </p> <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. You can display images from this repository using the following code:</p> <pre><code>{% include figure.html path="assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" %}</code></pre> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/iclr-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p> To ensure that there are no namespace conflicts, you must save your asset to your unique directory `/assets/img/2025-04-28-[SUBMISSION NAME]` within your submission. </p> <p> Please avoid using the direct HTML method of embedding images; they may not be properly resized. Some below complex ways to load images (note the different styles of the shapes/shadows): </p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/9-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/8-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/8.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/10-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/10.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/11-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/11.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/12-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/12.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-distill-example/7-1400.webp"/> <img src="/2025/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3>Interactive Figures</h3> <p> Here's how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work. All that's required is for you to export your figure into HTML format, and make sure that the file exists in the `assets/html/[SUBMISSION NAME]/` directory in this repository's root directory. To embed it into any page, simply insert the following code anywhere into your page. </p> <pre><code>{% include [FIGURE_NAME].html %}</code></pre> <p> For example, the following code can be used to generate the figure underneath it. </p> <pre><code class="language-python">import pandas as pd
import plotly.express as px

df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv')

fig = px.density_mapbox(
    df, lat='Latitude', lon='Longitude', z='Magnitude', radius=10,
    center=dict(lat=0, lon=180), zoom=0, mapbox_style="stamen-terrain")
fig.show()

fig.write_html('./assets/html/2025-04-28-distill-example/plotly_demo_1.html')
</code></pre> And then include it with the following: <pre><code class="language-html">&lt;div class="l-page"&gt;
  &lt;iframe src="{{ 'assets/html/2025-04-28-distill-example/plotly_demo_1.html' | relative_url }}" frameborder='0' scrolling='no' height="600px" width="100%"&gt;&lt;/iframe&gt;
&lt;/div&gt;
</code></pre> Voila! <div class="l-page"> <iframe src="/2025/assets/html/2025-04-28-distill-example/plotly_demo_1.html" frameborder='0' scrolling='no' height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p> Citations are then used in the article body with the <code>&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas. </p> <p> The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it. </p> <p> Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well - the authors are human and it's nice for them to have the community associate them with their work. </p> <h2 id="footnotes">Footnotes</h2> <p> Just wrap the text you would like to show up in a footnote in a <code>&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote> </p> <h2 id="code-blocks">Code Blocks</h2> <p> This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag as follows: </p> <pre><code>
{% highlight c++ linenos %}  <br/> code code code <br/> {% endhighlight %}

</code></pre> The keyword `linenos` triggers display of line numbers. You can try toggling it on or off yourself below: <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <h2 id="diagrams">Diagrams</h2> <p> This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams">jekyll-diagrams</a> plugin. Below, we generate a few examples of such diagrams using languages such as <a href="http://mermaid.js.org/">mermaid</a>, <a href="https://plantuml.com/">plantuml</a>, <a href="https://vega.github.io/vega-lite/">vega-lite</a>, etc. </p> <p> <b>Note</b>different diagram-generation packages require external dependencies to be installed on your machine. Also, be mindful of that because of diagram generation the first time you build your Jekyll website after adding new diagrams will be SLOW. For any other details, please refer to the <a href="https://github.com/zhustec/jekyll-diagrams">jekyll-diagrams</a> README. </p> <p> <b>Note:</b> This is not supported for local rendering! </p> <p> The diagram below was generated by the following code: </p> <pre><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice->>John: Hello John, how are you?
    John-->>Alice: Great!
{% endmermaid %}

</code></pre> <div class='jekyll-diagrams diagrams mermaid'> <svg id="mermaid-1732305559183" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1732305559183 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1732305559183 .node circle,#mermaid-1732305559183 .node ellipse,#mermaid-1732305559183 .node polygon,#mermaid-1732305559183 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1732305559183 .node.clickable{cursor:pointer}#mermaid-1732305559183 .arrowheadPath{fill:#333}#mermaid-1732305559183 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1732305559183 .edgeLabel{background-color:#e8e8e8}#mermaid-1732305559183 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1732305559183 .cluster text{fill:#333}#mermaid-1732305559183 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1732305559183 .actor{stroke:#ccf;fill:#ececff}#mermaid-1732305559183 text.actor{fill:#000;stroke:none}#mermaid-1732305559183 .actor-line{stroke:grey}#mermaid-1732305559183 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1732305559183 .messageLine0,#mermaid-1732305559183 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1732305559183 #arrowhead{fill:#333}#mermaid-1732305559183 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1732305559183 .messageText{fill:#333;stroke:none}#mermaid-1732305559183 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1732305559183 .labelText,#mermaid-1732305559183 .loopText{fill:#000;stroke:none}#mermaid-1732305559183 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1732305559183 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1732305559183 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1732305559183 .section{stroke:none;opacity:.2}#mermaid-1732305559183 .section0{fill:rgba(102,102,255,.49)}#mermaid-1732305559183 .section2{fill:#fff400}#mermaid-1732305559183 .section1,#mermaid-1732305559183 .section3{fill:#fff;opacity:.2}#mermaid-1732305559183 .sectionTitle0,#mermaid-1732305559183 .sectionTitle1,#mermaid-1732305559183 .sectionTitle2,#mermaid-1732305559183 .sectionTitle3{fill:#333}#mermaid-1732305559183 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1732305559183 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1732305559183 .grid path{stroke-width:0}#mermaid-1732305559183 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1732305559183 .task{stroke-width:2}#mermaid-1732305559183 .taskText{text-anchor:middle;font-size:11px}#mermaid-1732305559183 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1732305559183 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1732305559183 .taskText0,#mermaid-1732305559183 .taskText1,#mermaid-1732305559183 .taskText2,#mermaid-1732305559183 .taskText3{fill:#fff}#mermaid-1732305559183 .task0,#mermaid-1732305559183 .task1,#mermaid-1732305559183 .task2,#mermaid-1732305559183 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1732305559183 .taskTextOutside0,#mermaid-1732305559183 .taskTextOutside1,#mermaid-1732305559183 .taskTextOutside2,#mermaid-1732305559183 .taskTextOutside3{fill:#000}#mermaid-1732305559183 .active0,#mermaid-1732305559183 .active1,#mermaid-1732305559183 .active2,#mermaid-1732305559183 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1732305559183 .activeText0,#mermaid-1732305559183 .activeText1,#mermaid-1732305559183 .activeText2,#mermaid-1732305559183 .activeText3{fill:#000!important}#mermaid-1732305559183 .done0,#mermaid-1732305559183 .done1,#mermaid-1732305559183 .done2,#mermaid-1732305559183 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1732305559183 .doneText0,#mermaid-1732305559183 .doneText1,#mermaid-1732305559183 .doneText2,#mermaid-1732305559183 .doneText3{fill:#000!important}#mermaid-1732305559183 .crit0,#mermaid-1732305559183 .crit1,#mermaid-1732305559183 .crit2,#mermaid-1732305559183 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1732305559183 .activeCrit0,#mermaid-1732305559183 .activeCrit1,#mermaid-1732305559183 .activeCrit2,#mermaid-1732305559183 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1732305559183 .doneCrit0,#mermaid-1732305559183 .doneCrit1,#mermaid-1732305559183 .doneCrit2,#mermaid-1732305559183 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1732305559183 .activeCritText0,#mermaid-1732305559183 .activeCritText1,#mermaid-1732305559183 .activeCritText2,#mermaid-1732305559183 .activeCritText3,#mermaid-1732305559183 .doneCritText0,#mermaid-1732305559183 .doneCritText1,#mermaid-1732305559183 .doneCritText2,#mermaid-1732305559183 .doneCritText3{fill:#000!important}#mermaid-1732305559183 .titleText{text-anchor:middle;font-size:18px;fill:#000}
#mermaid-1732305559183 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1732305559183 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1732305559183 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1732305559183 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1732305559183 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1732305559183 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1732305559183 #compositionEnd,#mermaid-1732305559183 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1732305559183 #aggregationEnd,#mermaid-1732305559183 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1732305559183 #dependencyEnd,#mermaid-1732305559183 #dependencyStart,#mermaid-1732305559183 #extensionEnd,#mermaid-1732305559183 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1732305559183 .branch-label,#mermaid-1732305559183 .commit-id,#mermaid-1732305559183 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1732305559183{color:#000;font:normal normal 400 normal 16px / normal "Times New Roman"}</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg> </div> <h2 id="tweets">Tweets</h2> <p> An example of displaying a tweet: <div class='jekyll-twitter-plugin'><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </p> <p> An example of pulling from a timeline: <div class='jekyll-twitter-plugin'><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </p> <p> For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a> </p> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <h2 id="layouts">Layouts</h2> The main text column is referred to as the body. It's the assumed layout of any direct descendants of the `d-article` element. <div class="fake-img l-body"> <p>.l-body</p> </div> For images you want to display a little larger, try `.l-page`: <div class="fake-img l-page"> <p>.l-page</p> </div> All of these have an outset variant if you want to poke out from the body text a little bit. For instance: <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> Occasionally you'll want to use the full browser width. For this, use `.l-screen`. You can also inset the element a little from the edge of the browser by using the inset variant. <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of `.l-body`-sized text except on mobile screen sizes. <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <h2 id="other-typography">Other Typography?</h2> <p> Emphasis, aka italics, with the <code>&lt;i&gt;&lt;/i&gt;</code> tag <i>emphasis</i>. </p> <p> Strong emphasis, aka bold, with <code>&lt;b&gt;&lt;/b&gt;</code> tag <b>bold</b>. </p> <p> Strikethrough ca be accomplished with the <code>&lt;s&gt;&lt;/s&gt;</code> tag. <s>Scratch this.</s> </p> <ul> <li>First ordered list item</li> <li>Another item</li> <ol> <li>Unordered sub-list. </li> </ol> <li>And another item.</li> </ul> <p> For code, the language can be specified in the class. For example, use <q>language-javascript</q> for Javascript and <q>language-python</q> for Python code. </p> <pre><code class="language-javascript">var s = "JavaScript syntax highlighting";
  alert(s);</code></pre> <pre><code class="language-python">s = "Python syntax highlighting"
  print(s)</code></pre> <pre><code class="language-python">No language indicated, so no syntax highlighting.</code></pre> <p> A table can be created with the <code>&lt;table&gt;</code> element. Below is an example </p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p> <blockquote>Blockquotes can be defined with the &gt;blockquote&lt; tag.</blockquote> </p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry><entry><title type="html">An Illustrated Guide to Automatic Sparse Differentiation</title><link href="https://iclr-blogposts.github.io/2025/blog/sparse-autodiff/" rel="alternate" type="text/html" title="An Illustrated Guide to Automatic Sparse Differentiation"/><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://iclr-blogposts.github.io/2025/blog/sparse-autodiff</id><content type="html" xml:base="https://iclr-blogposts.github.io/2025/blog/sparse-autodiff/"><![CDATA[ <div style="display: none"> $$ \newcommand{\colorf}[1]{\textcolor{RoyalBlue}{#1}} \newcommand{\colorh}[1]{\textcolor{RedOrange}{#1}} \newcommand{\colorg}[1]{\textcolor{PineGreen}{#1}} \newcommand{\colorv}[1]{\textcolor{VioletRed}{#1}} \def\sR{\mathbb{R}} \def\vx{\mathbf{x}} \def\vv{\mathbf{v}} \def\vb{\mathbf{e}} \newcommand{\vvc}[1]{\colorv{\vv_{#1}}} \newcommand{\vbc}[1]{\colorv{\vb_{#1}}} \newcommand{\dfdx}[2]{\frac{\partial f_{#1}}{\partial x_{#2}}(\vx)} \newcommand{\J}[2]{J_{#1}(#2)} \def\Jf{\J{f}{\vx}} \def\Jg{\J{g}{\vx}} \def\Jh{\J{h}{g(\vx)}} \def\Jfc{\colorf{\Jf}} \def\Jgc{\colorg{\Jg}} \def\Jhc{\colorh{\Jh}} \newcommand{\D}[2]{D{#1}(#2)} \def\Df{\D{f}{\vx}} \def\Dg{\D{g}{\vx}} \def\Dh{\D{h}{g(\vx)}} \def\Dfc{\colorf{\Df}} \def\Dgc{\colorg{\Dg}} \def\Dhc{\colorh{\Dh}} $$ </div> <p>First-order optimization is ubiquitous in machine learning (ML) but second-order optimization is much less common. The intuitive reason is that high-dimensional vectors (gradients) are cheap, whereas high-dimensional matrices (Hessians) are expensive. Luckily, in numerous applications of ML to science or engineering, <strong>Hessians and Jacobians exhibit sparsity</strong>: most of their coefficients are known to be zero. Leveraging this sparsity can vastly <strong>accelerate automatic differentiation</strong> (AD) for Hessians and Jacobians, while decreasing its memory requirements <d-cite key="griewankEvaluatingDerivativesPrinciples2008"></d-cite>. Yet, while traditional AD is available in many high-level programming languages like Python <d-cite key="paszkePyTorchImperativeStyle2019"></d-cite> <d-cite key="bradburyJAXComposableTransformations2018"></d-cite> and Julia <d-cite key="sapienzaDifferentiableProgrammingDifferential2024"></d-cite>, <strong>automatic sparse differentiation (ASD) is not as widely used</strong>. One reason is that the underlying theory was developed outside of the ML research ecosystem, by people more familiar with low-level programming languages.</p> <p>With this blog post, we aim to shed light on the inner workings of ASD, bridging the gap between the ML and AD communities by presenting well established techniques from the latter field. We start out with a short introduction to traditional AD, covering the computation of Jacobians in both forward and reverse mode. We then dive into the two primary components of ASD: <strong>sparsity pattern detection</strong> and <strong>matrix coloring</strong>. Having described the computation of sparse Jacobians, we move on to sparse Hessians.<br/> We conclude with a practical demonstration of ASD, providing performance benchmarks and guidance on when to use ASD over AD.</p> <h2 id="automatic-differentiation">Automatic differentiation</h2> <p>Let us start by covering the fundamentals of traditional AD. The reader can find more details in the recent book by Blondel and Roulet <d-cite key="blondelElementsDifferentiableProgramming2024"></d-cite>, as well as Griewank and Walther <d-cite key="griewankEvaluatingDerivativesPrinciples2008"></d-cite>.</p> <p>AD makes use of the <strong>compositional structure</strong> of mathematical functions like deep neural networks. To make things simple, we will mainly look at a differentiable function $f$ composed of two differentiable functions $g: \mathbb{R}^{n} \rightarrow \mathbb{R}^{p}$ and $h: \mathbb{R}^{p} \rightarrow \mathbb{R}^{m}$, such that $f = h \circ g: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$. The insights gained from this toy example should translate directly to more deeply composed functions $f = g^{(L)} \circ g^{(L-1)} \circ \cdots \circ g^{(1)}$, and even computational graphs with more complex branching.</p> <h3 id="the-chain-rule">The chain rule</h3> <p>For a function $f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ and a point of linearization $\mathbf{x} \in \mathbb{R}^{n}$, the Jacobian $J_f(\mathbf{x})$ is the $m \times n$ matrix of first-order partial derivatives, whose $(i,j)$-th entry is</p> \[\big( \Jf \big)_{i,j} = \dfdx{i}{j} \in \sR \, .\] <p>For a composed function</p> \[\colorf{f} = \colorh{h} \circ \colorg{g} \, ,\] <p>the <strong>multivariate chain rule</strong> tells us that we obtain the Jacobian of $f$ by <strong>multiplying</strong> the Jacobians of $h$ and $g$:</p> \[\Jfc = \Jhc \cdot \Jgc .\] <p>Figure 1 illustrates this for $n=5$, $m=4$ and $p=3$. We will keep using these dimensions in following illustrations, even though the real benefits of ASD only appear as the dimension grows.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/chainrule_num.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/chainrule_num.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/chainrule_num.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/chainrule_num.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 1: Visualization of the multivariate chain rule for $f = h \circ g$. </div> <h3 id="ad-is-matrix-free">AD is matrix-free</h3> <p>We have seen how the chain rule translates the compositional structure of a function into the product structure of its Jacobian. In practice however, there is a problem: <strong>computing intermediate Jacobian matrices is inefficient and often impossible</strong>, especially with a dense matrix format. Examples of dense matrix formats include NumPy’s <code class="language-plaintext highlighter-rouge">ndarray</code>, PyTorch’s <code class="language-plaintext highlighter-rouge">Tensor</code>, JAX’s <code class="language-plaintext highlighter-rouge">Array</code> and Julia’s <code class="language-plaintext highlighter-rouge">Matrix</code>.</p> <p>As a motivating example, let us take a look at a tiny convolutional layer. We consider a convolutional filter of size $5 \times 5$, a single input channel and a single output channel. An input of size $28 \times 28 \times 1$ results in a $576 \times 784$ Jacobian, the structure of which is shown in Figure 2. All the white coefficients are <strong>structural zeros</strong>.</p> <p>If we represent the Jacobian of each convolutional layer as a dense matrix, we waste time computing coefficients which are mostly zero, and we waste memory storing those zero coefficients.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/big_conv_jacobian-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/big_conv_jacobian-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/big_conv_jacobian-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/big_conv_jacobian.png" class="img-70" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 2: Structure of the Jacobian of a tiny convolutional layer. </div> <p>In modern neural network architectures, which can contain over one trillion parameters, computing intermediate Jacobians is not only inefficient: it exceeds available memory. AD circumvents this limitation by using <strong>linear maps, lazy operators that act exactly like matrices</strong> but without explicitly storing every coefficient in memory.</p> <p>The differential $Df: \mathbf{x} \longmapsto Df(\mathbf{x})$ is a linear map which provides the best linear approximation of $f$ around a given point $\mathbf{x}$. We can rephrase the chain rule as a <strong>composition of linear maps</strong> instead of a product of matrices:</p> \[\Dfc = \colorf{\D{(h \circ g)}{\vx}} = \Dhc \circ \Dgc \, .\] <p>Note that all terms in this formulation of the chain rule are linear maps. A new visualization for our toy example can be found in Figure 3b. Our illustrations distinguish between matrices and linear maps by using solid and dashed lines respectively.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/chainrule_num.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/chainrule_num.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/chainrule_num.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/chainrule_num.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 3a: Chain rule using Jacobian matrices (solid outline). </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/matrixfree.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/matrixfree.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/matrixfree.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/matrixfree.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 3b: Chain rule using matrix-free linear maps (dashed outline). </div> <aside class="l-body box-note"> <p>We visualize “matrix entries” in linear maps to build intuition. Even though following illustrations will sometimes put numbers onto these entries, linear maps are best thought of as black-box operators.</p> </aside> <h3 id="forward-mode-ad">Forward-mode AD</h3> <p>Now that we have translated the compositional structure of our function $f$ into a compositional structure of linear maps, we can evaluate them by propagating vectors through them, one subfunction at a time.</p> <p>Figure 4 illustrates the propagation of a vector $\mathbf{v}_1 \in \mathbb{R}^n$ from the right-hand side. Since we propagate in the order of the original function evaluation ($g$ then $h$), this is called <strong>forward-mode AD</strong>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/forward_mode_eval.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/forward_mode_eval.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/forward_mode_eval.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/forward_mode_eval.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 4: Evaluating linear maps in forward mode. </div> <p>In the first step, we evaluate $Dg(\mathbf{x})(\mathbf{v}_1)$. Since this operation by definition corresponds to</p> \[\vvc{2} = \Dgc(\vvc{1}) = \Jgc \cdot \vvc{1} \;\in \sR^p \, ,\] <p>it is commonly called a <strong>Jacobian-vector product</strong> (JVP) or <strong>pushforward</strong>. The resulting vector $\mathbf{v}_2$ is then used to compute the subsequent JVP</p> \[\vvc{3} = \Dhc(\vvc{2}) = \Jhc \cdot \vvc{2} \;\in \sR^m \, ,\] <p>which in accordance with the chain rule is equivalent to</p> \[\vvc{3} = \Dfc(\vvc{1}) = \Jfc \cdot \vvc{1} \, ,\] <p>the JVP of our composed function $f$.</p> <p>The computational cost of one JVP of $f$ is approximately the same as the cost of one evaluation of $f$. <strong>Note that we did not compute intermediate Jacobian matrices at any point</strong> – we only propagated vectors through linear maps.</p> <h3 id="reverse-mode-ad">Reverse-mode AD</h3> <p>We can also propagate vectors through our linear maps from the left-hand side, resulting in <strong>reverse-mode AD</strong>, shown in Figure 5.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/reverse_mode_eval.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/reverse_mode_eval.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/reverse_mode_eval.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/reverse_mode_eval.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 5: Evaluating linear maps in reverse mode. </div> <p>This is commonly called a <strong>vector-Jacobian product</strong> (VJP) or <strong>pullback</strong>. Just like forward mode, the cost of one VJP of $f$ is approximately the same as the cost of one evaluation of $f$. Reverse mode is also matrix-free: <strong>no intermediate Jacobians are computed at any point</strong>.</p> <h3 id="from-linear-maps-back-to-jacobians">From linear maps back to Jacobians</h3> <p>This machinery can be used to turn a composed linear map (a lazy matrix representation) into a dense matrix in a computationally expensive process we call <strong>materialization</strong>. Counterintuitively, this process <strong>does not materialize any intermediate Jacobians</strong>.</p> <p>Figure 6 demonstrates how to <strong>materialize Jacobians column by column</strong> in forward mode. Evaluating the linear map $Df(\mathbf{x})$ on the $i$-th standard basis vector materializes the $i$-th column of the Jacobian $J_{f}(\mathbf{x})$:</p> \[\Dfc(\vbc{i}) = \left( \Jfc \right)_\colorv{i,:}\] <p>Thus, recovering the full $m \times n$ Jacobian requires one JVP with each of the $n$ standard basis vectors of the <strong>input space</strong>. Once again, we distinguish between linear maps and materialized matrices by using dashed and solid lines respectively:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/forward_mode.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/forward_mode.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/forward_mode.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/forward_mode.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 6: Forward-mode AD materializes Jacobians column-by-column. </div> <p>As illustrated in Figure 7, we can also <strong>materialize Jacobians row by row</strong> in reverse mode. Unlike forward mode in Figure 6, this requires one VJP with each of the $m$ standard basis vectors of the <strong>output space</strong>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/reverse_mode.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/reverse_mode.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/reverse_mode.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/reverse_mode.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 7: Reverse-mode AD materializes Jacobians row-by-row. </div> <p>These processes of materialization are computationally expensive due the fact that each JVP and VJP costs approximately as much as the evaluation of the function $f$ itself.</p> <aside class="l-body box-note"> <p>Since neural networks are usually trained using scalar loss functions, reverse-mode AD only requires the evaluation of a single VJP to materialize a gradient, which is rather cheap (see Baur and Strassen <d-cite key="baurComplexityPartialDerivatives1983"></d-cite>). This makes reverse-mode AD the method of choice for machine learners, who typically use the term backpropagation.</p> </aside> <h2 id="automatic-sparse-differentiation">Automatic sparse differentiation</h2> <h3 id="sparse-matrices">Sparse matrices</h3> <p>Sparse matrices are matrices in which most elements are zero. As shown in Figure 8, we refer to linear maps as “sparse linear maps” if they materialize to sparse matrices.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_matrix.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_matrix.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_matrix.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_matrix.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_map.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_map.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_map.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_map.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 8: A sparse Jacobian and its corresponding sparse linear map. </div> <p>When functions have many inputs and many outputs, a given output does not always depend on every single input. This endows the corresponding Jacobian with a <strong>sparsity pattern</strong>, where <strong>zero coefficients denote an absence of (first-order) dependency</strong>. The previous case of a convolutional layer is a simple example. An even simpler example is an activation function applied elementwise, for which the Jacobian is the identity matrix.</p> <h3 id="leveraging-sparsity">Leveraging sparsity</h3> <p>For now, we assume that the sparsity pattern of the Jacobian is always the same, regardless of the input, and that we know it ahead of time. We say that two columns or rows of the Jacobian matrix are <strong>structurally orthogonal</strong> if, for every index, at most one of them has a nonzero coefficient. In other words, the sparsity patterns of the columns are non-overlapping vectors, whose dot product is always zero regardless of their actual values.</p> <p>The core idea of ASD is that <strong>we can materialize multiple structurally orthogonal columns (or rows) with a single JVP (or VJP).</strong> This trick was first suggested in 1974 by Curtis, Powell and Reid <d-cite key="curtisEstimationSparseJacobian1974"></d-cite>. Since linear maps are additive, it always holds that for a set of basis vectors,</p> \[\Dfc(\vbc{i}+\ldots+\vbc{j}) = \underbrace{\Dfc(\vbc{i})}_{\left( \Jfc \right)_\colorv{i,:}} + \ldots + \underbrace{\Dfc(\vbc{j})}_{\left( \Jfc \right)_\colorv{j,:}} \, .\] <p>The components of the sum on the right-hand side each correspond to a column of the Jacobian. If these columns are known to be structurally orthogonal, the sum can be uniquely decomposed into its components, a process known as <strong>decompression</strong>. Thus, a single JVP is enough to retrieve the nonzero coefficients of several columns at once.</p> <p>This specific example using JVPs corresponds to forward-mode ASD and is visualized in Figure 9, where all structurally orthogonal columns have been colored in matching hues. By computing a single JVP with the vector $\mathbf{e}_1 + \mathbf{e}_2 + \mathbf{e}_5$, we obtain the sum of the first, second and fifth column of our Jacobian.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_ad.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_ad.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_ad.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_ad.svg" class="img-80" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 9: Materializing structurally orthogonal columns of a Jacobian in forward mode. </div> <p>A second JVP with the vector $\mathbf{e}_3 + \mathbf{e}_4$ gives us the sum of the remaining columns. We then assign the values in the resulting vectors back to the appropriate Jacobian entries. This final decompression step is shown in Figure 10.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_ad_forward_full.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_ad_forward_full.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_ad_forward_full.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_ad_forward_full.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_ad_forward_decompression.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_ad_forward_decompression.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_ad_forward_decompression.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_ad_forward_decompression.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 10: Materializing a Jacobian with forward-mode ASD: (a) compressed evaluation of orthogonal columns (b) decompression to Jacobian matrix </div> <p>The same idea can also be applied to reverse-mode AD, as shown in Figure 11. Instead of leveraging orthogonal columns, we rely on orthogonal rows. We can then materialize multiple rows in a single VJP.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_ad_reverse_full.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_ad_reverse_full.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_ad_reverse_full.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_ad_reverse_full.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_ad_reverse_decompression.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_ad_reverse_decompression.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_ad_reverse_decompression.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/sparse_ad_reverse_decompression.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 11: Materializing a Jacobian with reverse-mode ASD: (a) compressed evaluation of orthogonal rows (b) decompression to Jacobian matrix </div> <h3 id="pattern-detection-and-coloring">Pattern detection and coloring</h3> <p>Unfortunately, our initial assumption had a major flaw. Since AD only gives us a composition of linear maps and linear maps are black-box functions, the structure of the Jacobian is completely unknown. In other words, <strong>we cannot tell which rows and columns form structurally orthogonal groups</strong> without first materializing a Jacobian matrix. But if we materialize this Jacobian via traditional AD, then ASD isn’t necessary anymore.</p> <p>The solution to this problem is shown in Figure 12 (a): in order to find structurally orthogonal columns (or rows), we don’t need to materialize the full Jacobian. Instead, it is enough to <strong>detect the sparsity pattern</strong> of the Jacobian. This binary-valued pattern contains enough information to deduce structural orthogonality. From there, we use a <strong>coloring algorithm</strong> to group orthogonal columns (or rows) together. Such a coloring can be visualized on Figure 12 (b), where the yellow columns will be evaluated together (first JVP) and the light blue ones will be evaluated together (second JVP), for a total of 2 JVPs instead of 5.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparsity_pattern.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparsity_pattern.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparsity_pattern.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/sparsity_pattern.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/coloring.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/coloring.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/coloring.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/coloring.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 12: The first two steps of ASD: (a) sparsity pattern detection, (b) coloring of the sparsity pattern. </div> <p>To sum up, ASD consists of four steps:</p> <ol> <li>Pattern detection</li> <li>Coloring</li> <li>Compressed AD</li> <li>Decompression</li> </ol> <p>This compression-based pipeline is described at length by Gebremedhin, Manne and Pothen <d-cite key="gebremedhinWhatColorYour2005"></d-cite> in their landmark survey, or in Chapter 8 of the book by Griewank and Walther <d-cite key="griewankEvaluatingDerivativesPrinciples2008"></d-cite>. An alternative, direct pipeline is presented in Chapter 7 of the same book.</p> <p>We now discuss the first two steps in more detail. These steps can be much slower than a single call to the function $f$, but they are usually much faster than a full computation of the Jacobian with AD. This makes the sparse procedure worth it even for moderately large matrices. Additionally, if we need to compute Jacobians multiple times (for different inputs) and are able to reuse the sparsity pattern and the coloring result, <strong>the cost of this prelude can be amortized</strong> over several subsequent evaluations.</p> <h2 id="pattern-detection">Pattern detection</h2> <p>Mirroring the diversity of AD systems, there are also many possible approaches to sparsity pattern detection, each with its own advantages and tradeoffs. The work of Dixon <d-cite key="dixonAutomaticDifferentiationLarge1990"></d-cite> in the 1990’s was among the first of many papers on this subject, most of which can be classified into operator overloading or source transformation techniques. There are also ways to detect a sparsity pattern by probing the Jacobian coefficients with AD<d-cite key="griewankDetectingJacobianSparsity2002"></d-cite>, but we do not linger on them here.</p> <p>The method we present corresponds to a binary version of a forward-mode AD system, similar in spirit to <d-cite key="dixonAutomaticDifferentiationLarge1990"></d-cite> and <d-cite key="bischofEfficientComputationGradients1996"></d-cite>, in which performance is gained by representing matrix rows as index sets.</p> <h3 id="index-sets">Index sets</h3> <p>Our goal with sparsity pattern detection is to quickly compute the binary pattern of the Jacobian. One way to achieve better performance than traditional AD is to encode row sparsity patterns as index sets. The $i$-th row of the Jacobian is given by</p> \[\big( \Jf \big)_{i,:} = \left[\dfdx{i}{j}\right]_{1 \le j \le n} = \begin{bmatrix} \dfdx{i}{1} &amp; \ldots &amp; \dfdx{i}{n} \end{bmatrix} \, .\] <p>However, since we are only interested in the binary pattern</p> \[\left[\dfdx{i}{j} \neq 0\right]_{1 \le j \le n} \, ,\] <p>we can instead represent the sparsity pattern of the $i$-th row of a Jacobian by the corresponding <strong>index set of non-zero values</strong></p> \[\left\{j \;\Bigg|\; \dfdx{i}{j} \neq 0\right\} \, .\] <p>These equivalent sparsity pattern representations are illustrated in Figure 13. Each row index set tells us <strong>which inputs influenced a given output</strong>, at the first-order. For instance, output $i=2$ was influenced by inputs $j=4$ and $j=5$.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparsity_pattern_representations.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparsity_pattern_representations.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/sparsity_pattern_representations.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/sparsity_pattern_representations.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 13: Sparsity pattern representations: (a) original matrix, (b) binary pattern, (c) row index sets. </div> <h3 id="efficient-propagation">Efficient propagation</h3> <p>Figure 14 shows the traditional forward mode pass we want to avoid: propagating a full identity matrix through a linear map would materialize the Jacobian matrix of $f$, but also all intermediate linear maps. As previously discussed, this is not a viable option due to its inefficiency and high memory requirements.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/forward_mode_naive.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/forward_mode_naive.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/forward_mode_naive.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/forward_mode_naive.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 14: Propagating an identity matrix in forward mode to obtain the Jacobian. </div> <p>Instead, we initialize an input vector with index sets corresponding to the identity matrix. An alternative view on this vector is that it corresponds to the index set representation of the Jacobian of the input, since $\frac{\partial x_i}{\partial x_j} \neq 0$ only holds for $i=j$.</p> <p>Our goal is to propagate this index set such that we get an output vector of index sets that corresponds to the Jacobian sparsity pattern. This idea is visualized in Figure 15.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/forward_mode_sparse.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/forward_mode_sparse.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/forward_mode_sparse.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/forward_mode_sparse.svg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 15: Propagating an index set through a linear map to obtain a sparsity pattern. </div> <h3 id="abstract-interpretation">Abstract interpretation</h3> <p>Instead of going into implementation details, we want to provide some intuition on the second key ingredient of this typical forward-mode sparsity detection system: <strong>abstract interpretation</strong>.</p> <p>We will demonstrate this on a second toy example, the function</p> \[f(\vx) = \begin{bmatrix} x_1 x_2 + \text{sgn}(x_3)\\ \text{sgn}(x_3) \frac{x_4}{2} \end{bmatrix} \, .\] <p>The corresponding computational graph is shown in Figure 16, where circular nodes correspond to primitive functions, in this case addition, multiplication, division and the sign function. Scalar inputs $x_i$ and outputs $y_j$ are shown in rectangular nodes. Instead of evaluating the original compute graph for a given input $\mathbf{x}$, all inputs are seeded with their respective input index sets. Figure 16 annotates these index sets on the edges of the computational graph.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/compute_graph-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/compute_graph-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/compute_graph-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/compute_graph.png" class="img-90" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 16: Computational graph of the function $ f(\mathbf{x}) = x_1 + x_2x_3 + \text{sgn}(x_4) $, annotated with corresponding index sets. </div> <p>Abstract interpretation means that we imbue the computational graph with a different meaning. Instead of computing its output value, each primitive function must accumulate the index sets of its inputs, then propagate these index sets to the output, but only if the corresponding derivative is non-zero anywhere in the input domain.</p> <p>Since addition, multiplication and division globally have non-zero derivatives with respect to both of their inputs, the index sets of their inputs are accumulated and propagated. The sign function has a zero derivative for any input value. It therefore doesn’t propagate the index set of its input and instead returns an empty set.</p> <p>Figure 16 shows the resulting output index sets $\{1, 2\}$ and $\{4\}$ for outputs 1 and 2 respectively. These match the analytic Jacobian</p> \[J_f(\mathbf{x}) = \begin{bmatrix} x_2 &amp; x_1 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; \frac{\text{sgn}(x_3)}{2} \end{bmatrix} \, .\] <h3 id="local-and-global-patterns">Local and global patterns</h3> <p>The type of abstract interpretation shown above corresponds to <em>global sparsity detection</em>, computing index sets</p> \[\left\{j \;\Bigg|\; \dfdx{i}{j} \neq 0 \, \text{for some} \, \mathbf{x} \in \sR^{n} \right\}\] <p>that are valid over the entire input domain. Another type of abstract interpretation can be implemented, in which the original <em>primal computation</em> is propagated alongside index sets, computing</p> \[\left\{j \;\Bigg|\; \dfdx{i}{j} \neq 0 \right\}\] <p>for a specific input $\mathbf{x}$. These <em>local sparsity patterns</em> are strict subsets of global sparsity patterns, and can therefore result in fewer colors. However, they need to be recomputed when changing the input.</p> <h2 id="coloring">Coloring</h2> <p>Once we have detected a sparsity pattern, our next goal is to decide <strong>how to group the columns (or rows)</strong> of the Jacobian. The columns (or rows) in each group will be evaluated simultaneously using a single JVP (or VJP), with a linear combination of basis vectors called a <strong>seed</strong>. If the members of the group are structurally orthogonal, then this gives all the necessary information to retrieve every nonzero coefficient of the matrix.</p> <h3 id="graph-formulation">Graph formulation</h3> <p>Luckily, this grouping problem can be reformulated as graph coloring, which is very well studied. Let us build a graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ with vertex set $\mathcal{V}$ and edge set $\mathcal{E}$, such that each column is a vertex of the graph, and two vertices are connected if and only if their respective columns share a non-zero index. Put differently, an edge between vertices $j_1$ and $j_2$ means that columns $j_1$ and $j_2$ are not structurally orthogonal. Note that there are more efficient graph representations, summed up in <d-cite key="gebremedhinWhatColorYour2005"></d-cite>.</p> <p>We want to assign to each vertex $j$ a color $c(j)$, such that any two adjacent vertices $(j_1, j_2) \in \mathcal{E}$ have different colors $c(j_1) \neq c(j_2)$. This constraint ensures that columns in the same color group are indeed structurally orthogonal. If we can find a coloring which uses the smallest possible number of distinct colors, it will minimize the number of groups, and thus the computational cost of the AD step.</p> <p>Figure 17 shows an optimal coloring using two colors, whereas Figure 18 uses a suboptimal third color, requiring an extra JVP to materialize the Jacobian and therefore increasing the computational cost of ASD. Figure 19 shows an infeasible coloring: vertices 2 and 4 on the graph are adjacent, but share a color. This results in overlapping columns.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/colored_graph.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/colored_graph.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/colored_graph.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/colored_graph.svg" class="img-80" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 17: Optimal graph coloring. </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/colored_graph_suboptimal.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/colored_graph_suboptimal.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/colored_graph_suboptimal.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/colored_graph_suboptimal.svg" class="img-80" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 18: Suboptimal graph coloring (vertex 1 could be colored in yellow). </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/colored_graph_infeasible.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/colored_graph_infeasible.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/colored_graph_infeasible.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/colored_graph_infeasible.svg" class="img-80" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 19: Infeasible graph coloring (vertices 2 and 4 are adjacent on the graph, but share a color). </div> <p>If we perform column coloring, forward-mode AD is required, while reverse-mode AD is needed for row coloring.</p> <h3 id="greedy-algorithm">Greedy algorithm</h3> <p>Unfortunately, the graph coloring problem is NP-hard: there is currently no way to solve it in polynomial time for every instance. The optimal solution is known only for specific patterns, such as banded matrices. However, efficient heuristics exist which generate good enough solutions in reasonable time. The most widely used heuristic is the greedy algorithm, which processes vertices one after the other. This algorithm assigns to each vertex the smallest color that is not already present among its neighbors, and it never backtracks. A crucial hyperparameter is the choice of ordering, for which various criteria have been proposed <d-cite key="gebremedhinColPackSoftwareGraph2013"></d-cite>.</p> <h3 id="bicoloring">Bicoloring</h3> <p>A more advanced coloring technique called <strong>bicoloring</strong> allows combining forward and reverse modes, because the recovery of the Jacobian leverages both columns (JVPs) and rows (VJPs) <d-cite key="hossainComputingSparseJacobian1998"></d-cite> <d-cite key="colemanEfficientComputationSparse1998"></d-cite>.</p> <p>Figure 20 shows bicoloring on a toy example in which no pair of columns or rows is structurally orthogonal. Even with ASD, materializing the Jacobian would require $5$ JVPs in forward-mode or $4$ VJPs in reverse mode. However, if we use both modes simultaneously, we can materialize the full Jacobian by computing only $1$ JVP and $1$ VJP.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/bicoloring.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/bicoloring.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/bicoloring.svg-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/bicoloring.svg" class="img-50" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 20: Bicoloring on a toy example with a dense row and a dense column. </div> <h2 id="second-order">Second order</h2> <p>While first-order automatic differentiation AD focuses on computing the gradient or Jacobian, second-order AD extends the same ideas to the <strong>Hessian</strong> matrix</p> \[\nabla^2 f (\mathbf{x}) = \left(\frac{\partial^2 f(\mathbf{x})}{\partial x_i ~ \partial x_j} \right)_{i,j} \, .\] <p>The Hessian contains second-order partial derivatives of a scalar function, essentially capturing the curvature of the function at a point. This is particularly relevant in <strong>optimization</strong>, where the Hessian provides crucial information about the function’s local behavior. Specifically, the Hessian allows us to distinguish between local minima, maxima, and saddle points. By incorporating second-order information, optimization algorithms converge more robustly where the gradient alone doesn’t provide enough information for effective search directions.</p> <h3 id="hessian-vector-products">Hessian-vector products</h3> <p>For second-order AD, the key subroutine is the <strong>Hessian-vector product (HVP)</strong>. The Hessian is the Jacobian matrix of the gradient function $\nabla f: \mathbf{x} \mapsto \nabla f(\mathbf{x})$, that is,</p> \[\nabla^2 f (\mathbf{x}) = J_{\nabla f}(\mathbf{x}) \, .\] <p>An HVP computes the product of the Hessian matrix with a vector, which can be viewed as the JVP of the gradient function.</p> \[\nabla^2 f(\mathbf{x}) (\mathbf{v}) = D[\nabla f](\mathbf{x})(\mathbf{v})\] <p>Note that the gradient function is itself computed via a VJP of $f$. Thus, the HVP approach we described computes the JVP of a VJP, giving it the name “forward over reverse”. In forward-over-reverse HVPs, the complexity of a single product scales roughly with the complexity of the function $f$ itself. This procedure was first considered by Pearlmutter <d-cite key="pearlmutterFastExactMultiplication1994"></d-cite> and recently revisited in a 2024 ICLR blog post <d-cite key="dagreouHowComputeHessianvector2024"></d-cite>. Note that other mode combinations are possible, like “forward over forward”, which has a higher complexity but is less expensive in terms of storage.</p> <p>The Hessian has a <strong>symmetric</strong> structure (equal to its transpose), which means that matrix-vector products and vector-matrix products coincide. This explains why we don’t need a VHP in addition to the HVP. This specificity can be exploited in the sparsity detection as well as in the coloring phase.</p> <h3 id="second-order-pattern-detection">Second order pattern detection</h3> <p>Detecting the sparsity pattern of the Hessian is more complicated than for the Jacobian. This is because, in addition to the usual linear dependencies, we now have to account for <strong>nonlinear interactions</strong> in the computational graph. The operator overloading method of Walther <d-cite key="waltherComputingSparseHessians2008"></d-cite> was a pioneering effort towards Hessian sparsity detection, although more efficient alternatives quickly emerged <d-cite key="gowerNewFrameworkComputation2012"></d-cite>.</p> <p>For instance, if $f(\mathbf{x})$ involves a term of the form $x_1 + x_2$, it will not directly affect the Hessian. However, we cannot ignore this term, since multiplying it with $x_3$ to obtain an output $f(\mathbf{x}) = (x_1 + x_2)\,x_3$ will yield non-zero coefficients at positions $(1, 3)$, $(3, 1)$, $(2, 3)$ and $(3, 2)$. Thus, the abstract interpretation system used for second-order pattern detection needs a finer classification of primitive functions. It must distinguish between locally constant, locally linear, and locally nonlinear behavior in each argument, and distinguish between zero and non-zero cross-derivatives.</p> <h3 id="symmetric-coloring">Symmetric coloring</h3> <p>When it comes to graph coloring for the Hessian, there are more options for decompression thanks to symmetry. Even if two columns in the Hessian are not structurally orthogonal, missing coefficients can be recovered by leveraging the corresponding rows instead of relying solely on the columns. In other words, if $H_{ij}$ is lost during compression because of colliding nonzero coefficients, there is still a chance to retrieve it through $H_{ji}$. This backup storage enables the use of <strong>fewer distinct colors</strong>, reducing the complexity of the AD part compared to traditional row or column coloring.</p> <p>Powell and Toint <d-cite key="powellEstimationSparseHessian1979"></d-cite> were the first to notice symmetry-related optimizations, before Coleman and Moré <d-cite key="colemanEstimationSparseHessian1984"></d-cite> made the connection to graph coloring explicit. While symmetric coloring and decompression are more computationally expensive than their nonsymmetric counterparts, this cost is typically negligible compared to the savings we get from fewer HVPs.</p> <h2 id="demonstration">Demonstration</h2> <p>We complement this tutorial with a demonstration of automatic sparse differentiation in a high-level programming language, namely the <a href="https://julialang.org/">Julia language</a> <d-cite key="bezansonJuliaFreshApproach2017"></d-cite>. While still at an early stage of development, we hope that such an example of unified pipeline for sparse Jacobians and Hessians can inspire developers in other languages to revisit ASD.</p> <aside class="l-body box-note"> <p>The authors of this blog post are all developers of the ASD ecosystem in Julia. We are not aware of a similar ecosystem in Python or R, which is why we chose Julia to present it. The closest counterpart we know is coded in C, namely the combination of ADOL-C <d-cite key="waltherGettingStartedADOLC2009"></d-cite> and ColPack <d-cite key="gebremedhinColPackSoftwareGraph2013"></d-cite>.</p> </aside> <h3 id="necessary-packages">Necessary packages</h3> <p>Here are the packages we will use for this demonstration.</p> <ul> <li><a href="https://github.com/adrhill/SparseConnectivityTracer.jl">SparseConnectivityTracer.jl</a> <d-cite key="hillSparseConnectivityTracerjl2024"></d-cite>: Sparsity pattern detection with operator overloading.</li> <li><a href="https://github.com/gdalle/SparseMatrixColorings.jl">SparseMatrixColorings.jl</a> <d-cite key="dalleGdalleSparseMatrixColoringsjlV04102024"></d-cite>: Greedy algorithms for colorings, decompression utilities.</li> <li><a href="https://github.com/JuliaDiff/ForwardDiff.jl">ForwardDiff.jl</a> <d-cite key="revelsForwardModeAutomaticDifferentiation2016"></d-cite>: Forward-mode AD and computation of JVPs.</li> <li><a href="https://github.com/JuliaDiff/DifferentiationInterface.jl">DifferentiationInterface.jl</a> <d-cite key="dalleJuliaDiffDifferentiationInterfacejlDifferentiationInterfacev06232024"></d-cite>: High-level interface bringing all of these together, originally inspired by <d-cite key="schaferAbstractDifferentiationjlBackendAgnosticDifferentiable2022"></d-cite>.</li> </ul> <p>This modular pipeline comes as a replacement and extension of a previous ASD system in Julia <d-cite key="gowdaSparsityProgrammingAutomated2019"></d-cite>. We also use a few other packages for data manipulation <d-cite key="bouchet-valatDataFramesjlFlexibleFast2023"></d-cite> and visualization <d-cite key="danischMakiejlFlexibleHighperformance2021"></d-cite>.</p> <p>Like in any other language, the first step is importing the dependencies:</p> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">DifferentiationInterface</span>
<span class="k">using</span> <span class="n">SparseConnectivityTracer</span><span class="x">,</span> <span class="n">SparseMatrixColorings</span>
<span class="k">import</span> <span class="n">ForwardDiff</span>
</code></pre></div></div> <h3 id="test-function">Test function</h3> <p>As our test function, we choose a very simple iterated difference operator. It takes a vector $\mathbf{x} \in \mathbb{R}^n$ and outputs a slightly shorter vector $y \in \mathbb{R}^{n-k}$ depending on the number of iterations $k$. In pure Julia, this is written as follows (using the built-in <code class="language-plaintext highlighter-rouge">diff</code> recursively):</p> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">function</span><span class="nf"> iter_diff</span><span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="n">k</span><span class="x">)</span>
    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">else</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">iter_diff</span><span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="n">k</span> <span class="o">-</span> <span class="mi">1</span><span class="x">)</span>
        <span class="k">return</span> <span class="n">diff</span><span class="x">(</span><span class="n">y</span><span class="x">)</span>
    <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div> <p>Let us check that the function returns what we expect:</p> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">julia</span><span class="o">&gt;</span> <span class="n">iter_diff</span><span class="x">([</span><span class="mi">1</span><span class="x">,</span> <span class="mi">4</span><span class="x">,</span> <span class="mi">9</span><span class="x">,</span> <span class="mi">16</span><span class="x">],</span> <span class="mi">1</span><span class="x">)</span>
<span class="mi">3</span><span class="o">-</span><span class="n">element</span> <span class="kt">Vector</span><span class="x">{</span><span class="kt">Int64</span><span class="x">}</span><span class="o">:</span>
 <span class="mi">3</span>
 <span class="mi">5</span>
 <span class="mi">7</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">iter_diff</span><span class="x">([</span><span class="mi">1</span><span class="x">,</span> <span class="mi">4</span><span class="x">,</span> <span class="mi">9</span><span class="x">,</span> <span class="mi">16</span><span class="x">],</span> <span class="mi">2</span><span class="x">)</span>
<span class="mi">2</span><span class="o">-</span><span class="n">element</span> <span class="kt">Vector</span><span class="x">{</span><span class="kt">Int64</span><span class="x">}</span><span class="o">:</span>
 <span class="mi">2</span>
 <span class="mi">2</span>
</code></pre></div></div> <h3 id="backend-switch">Backend switch</h3> <p>The key concept behind DifferentiationInterface.jl is that of <strong>backends</strong>. There are several AD systems in Julia, each with different features and tradeoff, that can be accessed through a common API. Here, we use ForwardDiff.jl as our AD backend:</p> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ad</span> <span class="o">=</span> <span class="n">AutoForwardDiff</span><span class="x">()</span>
</code></pre></div></div> <p>To build an ASD backend, we bring together three ingredients corresponding to each step:</p> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sparsity_detector</span> <span class="o">=</span> <span class="n">TracerSparsityDetector</span><span class="x">()</span>  <span class="c"># from SparseConnectivityTracer</span>
<span class="n">coloring_algorithm</span> <span class="o">=</span> <span class="n">GreedyColoringAlgorithm</span><span class="x">()</span>  <span class="c"># from SparseMatrixColorings</span>
<span class="n">asd</span> <span class="o">=</span> <span class="n">AutoSparse</span><span class="x">(</span><span class="n">ad</span><span class="x">;</span> <span class="n">sparsity_detector</span><span class="x">,</span> <span class="n">coloring_algorithm</span><span class="x">)</span>
</code></pre></div></div> <h3 id="jacobian-computation">Jacobian computation</h3> <p>We can now compute the Jacobian of <code class="language-plaintext highlighter-rouge">iter_diff</code> (with respect to $\mathbf{x}$) using either backend, and compare the results. Just like AD, ASD is fully automatic. It doesn’t require the user to change any code besides specifying a backend:</p> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">julia</span><span class="o">&gt;</span> <span class="n">x</span><span class="x">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">rand</span><span class="x">(</span><span class="mi">10</span><span class="x">),</span> <span class="mi">3</span><span class="x">;</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">jacobian</span><span class="x">(</span><span class="n">iter_diff</span><span class="x">,</span> <span class="n">ad</span><span class="x">,</span> <span class="n">x</span><span class="x">,</span> <span class="n">Constant</span><span class="x">(</span><span class="n">k</span><span class="x">))</span>
<span class="mi">7</span><span class="n">×10</span> <span class="kt">Matrix</span><span class="x">{</span><span class="kt">Float64</span><span class="x">}</span><span class="o">:</span>
 <span class="o">-</span><span class="mf">1.0</span>   <span class="mf">3.0</span>  <span class="o">-</span><span class="mf">3.0</span>   <span class="mf">1.0</span>   <span class="mf">0.0</span>   <span class="mf">0.0</span>   <span class="mf">0.0</span>   <span class="mf">0.0</span>   <span class="mf">0.0</span>  <span class="mf">0.0</span>
  <span class="mf">0.0</span>  <span class="o">-</span><span class="mf">1.0</span>   <span class="mf">3.0</span>  <span class="o">-</span><span class="mf">3.0</span>   <span class="mf">1.0</span>   <span class="mf">0.0</span>   <span class="mf">0.0</span>   <span class="mf">0.0</span>   <span class="mf">0.0</span>  <span class="mf">0.0</span>
  <span class="mf">0.0</span>   <span class="mf">0.0</span>  <span class="o">-</span><span class="mf">1.0</span>   <span class="mf">3.0</span>  <span class="o">-</span><span class="mf">3.0</span>   <span class="mf">1.0</span>   <span class="mf">0.0</span>   <span class="mf">0.0</span>   <span class="mf">0.0</span>  <span class="mf">0.0</span>
  <span class="mf">0.0</span>   <span class="mf">0.0</span>   <span class="mf">0.0</span>  <span class="o">-</span><span class="mf">1.0</span>   <span class="mf">3.0</span>  <span class="o">-</span><span class="mf">3.0</span>   <span class="mf">1.0</span>   <span class="mf">0.0</span>   <span class="mf">0.0</span>  <span class="mf">0.0</span>
  <span class="mf">0.0</span>   <span class="mf">0.0</span>   <span class="mf">0.0</span>   <span class="mf">0.0</span>  <span class="o">-</span><span class="mf">1.0</span>   <span class="mf">3.0</span>  <span class="o">-</span><span class="mf">3.0</span>   <span class="mf">1.0</span>   <span class="mf">0.0</span>  <span class="mf">0.0</span>
  <span class="mf">0.0</span>   <span class="mf">0.0</span>   <span class="mf">0.0</span>   <span class="mf">0.0</span>   <span class="mf">0.0</span>  <span class="o">-</span><span class="mf">1.0</span>   <span class="mf">3.0</span>  <span class="o">-</span><span class="mf">3.0</span>   <span class="mf">1.0</span>  <span class="mf">0.0</span>
  <span class="mf">0.0</span>   <span class="mf">0.0</span>   <span class="mf">0.0</span>   <span class="mf">0.0</span>   <span class="mf">0.0</span>   <span class="mf">0.0</span>  <span class="o">-</span><span class="mf">1.0</span>   <span class="mf">3.0</span>  <span class="o">-</span><span class="mf">3.0</span>  <span class="mf">1.0</span>

<span class="n">julia</span><span class="o">&gt;</span> <span class="n">jacobian</span><span class="x">(</span><span class="n">iter_diff</span><span class="x">,</span> <span class="n">asd</span><span class="x">,</span> <span class="n">x</span><span class="x">,</span> <span class="n">Constant</span><span class="x">(</span><span class="n">k</span><span class="x">))</span>
<span class="mi">7</span><span class="n">×10</span> <span class="n">SparseArrays</span><span class="o">.</span><span class="kt">SparseMatrixCSC</span><span class="x">{</span><span class="kt">Float64</span><span class="x">,</span> <span class="kt">Int64</span><span class="x">}</span> <span class="n">with</span> <span class="mi">28</span> <span class="n">stored</span> <span class="n">entries</span><span class="o">:</span>
 <span class="o">-</span><span class="mf">1.0</span>   <span class="mf">3.0</span>  <span class="o">-</span><span class="mf">3.0</span>   <span class="mf">1.0</span>    <span class="n">⋅</span>     <span class="n">⋅</span>     <span class="n">⋅</span>     <span class="n">⋅</span>     <span class="n">⋅</span>    <span class="n">⋅</span> 
   <span class="n">⋅</span>   <span class="o">-</span><span class="mf">1.0</span>   <span class="mf">3.0</span>  <span class="o">-</span><span class="mf">3.0</span>   <span class="mf">1.0</span>    <span class="n">⋅</span>     <span class="n">⋅</span>     <span class="n">⋅</span>     <span class="n">⋅</span>    <span class="n">⋅</span> 
   <span class="n">⋅</span>     <span class="n">⋅</span>   <span class="o">-</span><span class="mf">1.0</span>   <span class="mf">3.0</span>  <span class="o">-</span><span class="mf">3.0</span>   <span class="mf">1.0</span>    <span class="n">⋅</span>     <span class="n">⋅</span>     <span class="n">⋅</span>    <span class="n">⋅</span> 
   <span class="n">⋅</span>     <span class="n">⋅</span>     <span class="n">⋅</span>   <span class="o">-</span><span class="mf">1.0</span>   <span class="mf">3.0</span>  <span class="o">-</span><span class="mf">3.0</span>   <span class="mf">1.0</span>    <span class="n">⋅</span>     <span class="n">⋅</span>    <span class="n">⋅</span> 
   <span class="n">⋅</span>     <span class="n">⋅</span>     <span class="n">⋅</span>     <span class="n">⋅</span>   <span class="o">-</span><span class="mf">1.0</span>   <span class="mf">3.0</span>  <span class="o">-</span><span class="mf">3.0</span>   <span class="mf">1.0</span>    <span class="n">⋅</span>    <span class="n">⋅</span> 
   <span class="n">⋅</span>     <span class="n">⋅</span>     <span class="n">⋅</span>     <span class="n">⋅</span>     <span class="n">⋅</span>   <span class="o">-</span><span class="mf">1.0</span>   <span class="mf">3.0</span>  <span class="o">-</span><span class="mf">3.0</span>   <span class="mf">1.0</span>   <span class="n">⋅</span> 
   <span class="n">⋅</span>     <span class="n">⋅</span>     <span class="n">⋅</span>     <span class="n">⋅</span>     <span class="n">⋅</span>     <span class="n">⋅</span>   <span class="o">-</span><span class="mf">1.0</span>   <span class="mf">3.0</span>  <span class="o">-</span><span class="mf">3.0</span>  <span class="mf">1.0</span>
</code></pre></div></div> <p>In one case, we get a dense matrix, in the other it is sparse. Note that in Julia, linear algebra operations are optimized for sparse matrices, which means this format can be beneficial for downstream use. We now show that sparsity also unlocks faster computation of the Jacobian itself.</p> <h3 id="preparation">Preparation</h3> <p>Sparsity pattern detection and matrix coloring are performed in a so-called “preparation step”, whose output can be <strong>reused across several calls</strong> to <code class="language-plaintext highlighter-rouge">jacobian</code> (as long as the pattern stays the same).</p> <p>Thus, to extract more performance, we can create this object only once</p> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prep</span> <span class="o">=</span> <span class="n">prepare_jacobian</span><span class="x">(</span><span class="n">iter_diff</span><span class="x">,</span> <span class="n">sparse_backend</span><span class="x">,</span> <span class="n">x</span><span class="x">,</span> <span class="n">Constant</span><span class="x">(</span><span class="n">k</span><span class="x">))</span>
</code></pre></div></div> <p>and then reuse it as much as possible, for instance inside the loop of an iterative algorithm (note the additional <code class="language-plaintext highlighter-rouge">prep</code> argument):</p> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">jacobian</span><span class="x">(</span><span class="n">iter_diff</span><span class="x">,</span> <span class="n">prep</span><span class="x">,</span> <span class="n">sparse_backend</span><span class="x">,</span> <span class="n">x</span><span class="x">,</span> <span class="n">Constant</span><span class="x">(</span><span class="n">k</span><span class="x">))</span>
</code></pre></div></div> <p>Inside the preparation result, we find the output of sparsity pattern detection</p> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">julia</span><span class="o">&gt;</span> <span class="n">sparsity_pattern</span><span class="x">(</span><span class="n">prep</span><span class="x">)</span>
<span class="mi">7</span><span class="n">×10</span> <span class="n">SparseArrays</span><span class="o">.</span><span class="kt">SparseMatrixCSC</span><span class="x">{</span><span class="kt">Bool</span><span class="x">,</span> <span class="kt">Int64</span><span class="x">}</span> <span class="n">with</span> <span class="mi">28</span> <span class="n">stored</span> <span class="n">entries</span><span class="o">:</span>
 <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="n">⋅</span>
 <span class="n">⋅</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="n">⋅</span>
 <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="n">⋅</span>
 <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="n">⋅</span>
 <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="n">⋅</span>  <span class="n">⋅</span>
 <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="n">⋅</span>
 <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="n">⋅</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>  <span class="mi">1</span>
</code></pre></div></div> <p>and the coloring of the columns:</p> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">julia</span><span class="o">&gt;</span> <span class="n">column_colors</span><span class="x">(</span><span class="n">prep</span><span class="x">)</span>
<span class="mi">10</span><span class="o">-</span><span class="n">element</span> <span class="kt">Vector</span><span class="x">{</span><span class="kt">Int64</span><span class="x">}</span><span class="o">:</span>
 <span class="mi">1</span>
 <span class="mi">2</span>
 <span class="mi">3</span>
 <span class="mi">4</span>
 <span class="mi">1</span>
 <span class="mi">2</span>
 <span class="mi">3</span>
 <span class="mi">4</span>
 <span class="mi">1</span>
 <span class="mi">2</span>
</code></pre></div></div> <p>Note that it uses only $c = 4$ different colors, which means we need $4$ JVPs instead of the initial $n = 10$ to reconstruct the Jacobian.</p> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">julia</span><span class="o">&gt;</span> <span class="n">ncolors</span><span class="x">(</span><span class="n">prep</span><span class="x">)</span>
<span class="mi">4</span>
</code></pre></div></div> <h3 id="coloring-visualization">Coloring visualization</h3> <p>We just saw that there is a discrepancy between the number of different colors $c$ and the input size $n$. This ratio $n / c$ typically gets larger as the input grows, which makes sparse differentiation more and more competitive.</p> <p>We illustrate this with the Jacobians of <code class="language-plaintext highlighter-rouge">iter_diff</code> for several values of $n$ and $k$:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/demo/banded-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/demo/banded-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/demo/banded-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/demo/banded.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 21: Coloring numbers are often agnostic to the input size. </div> <p>The main takeaway of Figure 21 is that in this case, <strong>the number of colors does not depend on the dimension</strong> $n$, only on the number of iterations $k$. In fact, <code class="language-plaintext highlighter-rouge">iter_diff</code> with $k$ iterations gives rise to a banded Jacobian with $k+1$ bands, for which we can easily verify that the optimal coloring uses as many colors as bands, i.e. $c = k+1$. For this particular case, the greedy coloring also happens to find the optimal solution.</p> <h3 id="performance-benefits">Performance benefits</h3> <p>Here we present a benchmark for the Jacobian of <code class="language-plaintext highlighter-rouge">iter_diff</code> with varying $n$ and fixed $k$. Our goal is to find out when sparse differentiation becomes relevant. Benchmark data can be generated with the following code:</p> <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">DifferentiationInterfaceTest</span>
<span class="n">scenarios</span> <span class="o">=</span> <span class="x">[</span>
    <span class="n">Scenario</span><span class="x">{</span><span class="o">:</span><span class="n">jacobian</span><span class="x">,</span> <span class="o">:</span><span class="n">out</span><span class="x">}(</span><span class="n">iter_diff</span><span class="x">,</span> <span class="n">rand</span><span class="x">(</span><span class="n">n</span><span class="x">);</span> <span class="n">contexts</span><span class="o">=</span><span class="x">(</span><span class="n">Constant</span><span class="x">(</span><span class="n">k</span><span class="x">),))</span>
    <span class="k">for</span> <span class="n">n</span> <span class="k">in</span> <span class="n">round</span><span class="o">.</span><span class="x">(</span><span class="kt">Int</span><span class="x">,</span> <span class="mi">10</span> <span class="o">.^</span> <span class="x">(</span><span class="mi">1</span><span class="o">:</span><span class="mf">0.3</span><span class="o">:</span><span class="mi">4</span><span class="x">))</span>
<span class="x">]</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">benchmark_differentiation</span><span class="x">(</span>
    <span class="x">[</span><span class="n">ad</span><span class="x">,</span> <span class="n">asd</span><span class="x">],</span> <span class="n">scenarios</span><span class="x">;</span> <span class="n">benchmark</span><span class="o">=:</span><span class="n">full</span>
<span class="x">)</span>
</code></pre></div></div> <p>It gives rise to the following performance curves (lower is better):</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/demo/benchmark-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/demo/benchmark-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2025/assets/img/2025-04-28-sparse-autodiff/demo/benchmark-1400.webp"/> <img src="/2025/assets/img/2025-04-28-sparse-autodiff/demo/benchmark.png" class="img-90" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 22: Performance benefits of sparsity </div> <p>As we can see on Figure 22, there are three main regimes:</p> <ol> <li>For very small inputs, we gain nothing by leveraging sparsity.</li> <li>For medium-sized inputs, sparsity handling is only useful if we can amortize the cost of detection and coloring.</li> <li>For very large inputs, even the overhead of detection and coloring is worth paying as part of a sparse Jacobian computation.</li> </ol> <p>Importantly, ASD can yield an <strong>asymptotic speedup</strong> compared to AD, not just a constant one. In our test case, the cost of a JVP for <code class="language-plaintext highlighter-rouge">iter_diff</code> scales with $kn$. Sparse differentiation requires $c$ JVPs instead of $n$, so with $c = k+1$ here its total cost scales as $\Theta(k^2 n)$ instead of $\Theta(k n^2)$. Thus, on the log-log plot of Figure 22, the ASD curve (without detection) has a slope of $1$ while the AD curve has a slope of $2$.</p> <p>Although the specific thresholds between regimes are problem-dependent, our conclusions hold in general.</p> <h2 id="conclusion">Conclusion</h2> <p>By now, the reader should have a better understanding of how sparsity can be used for efficient differentiation.</p> <p>But should it always be used? Here are a list of criteria to consider when choosing between AD and ASD:</p> <ul> <li><strong>Which derivative is needed?</strong> When computing gradients of scalar functions using reverse mode, sparsity can’t be leveraged, as only a single VJP is required. In practice, ASD only speeds up derivatives like the Jacobian and Hessian which have a matrix form.</li> <li><strong>What operations will be performed on the derivative matrix?</strong> For a single matrix-vector product $J \mathbf{v}$, the linear map will always be faster. But if we want to solve linear systems $J \mathbf{v} = \mathbf{y}$, then it may be useful to compute the full matrix first to leverage sparse factorization routines.</li> <li><strong>How expensive is the function at hand?</strong> This directly impacts the cost of a JVP, VJP or HVP, which scales with the cost of one function evaluation.</li> <li><strong>How sparse would the matrix be?</strong> This dictates the efficiency of sparsity detection and coloring, as well as the number of matrix-vector products necessary. While it may be hard to get an exact estimate, concepts like partial separability can help provide upper bounds <d-cite key="gowerComputingSparsityPattern2014"></d-cite>. In general, the relation between the number of colors $c$ and the dimension $n$ is among the most crucial quantities to analyze.</li> </ul> <p>In simple settings (like finite differences, which create banded Jacobians), the sparsity pattern and optimal coloring can be derived manually. But as soon as the function becomes more complex, <strong>automating this process becomes essential</strong> to ensure wide usability. We hope that the exposition above will motivate the implementation of user-friendly ASD in a variety of programming languages and AD frameworks.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[In numerous applications of machine learning, Hessians and Jacobians exhibit sparsity, a property that can be leveraged to vastly accelerate their computation. While the usage of automatic differentiation in machine learning is ubiquitous, automatic sparse differentiation (ASD) remains largely unknown. This post introduces ASD, explaining its key components and their roles in the computation of both sparse Jacobians and Hessians. We conclude with a practical demonstration showcasing the performance benefits of ASD.]]></summary></entry></feed>